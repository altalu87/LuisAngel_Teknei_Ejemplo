{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d05d661-b5b6-4dd1-9483-b2350a9f2bb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Requisitos técnicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d021d3-0158-4ce4-9b4a-7be4a78b7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !Pip install pyspark\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701be47-0720-4809-9bec-44ed3230dfd0",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476c2a7a-ace1-49a0-8333-445c9448535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la sessión con spark\n",
    "from SparkDataLoader import SparkDataLoader\n",
    "from SparkDataCleaner import SparkDataCleaner\n",
    "from SparkDataExporter import SparkDataExporter\n",
    "from pyspark.sql import functions as F\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d7839-ff63-4f2e-b2a7-f2c6f1ea03d3",
   "metadata": {},
   "source": [
    "# Lectura, carga o importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1aa6efa-e5d4-4399-a67b-86b481317931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:44:33,206 - SparkDataLoader - INFO - Inicializando SparkSession: StarSchemaLoader\n",
      "2025-11-06 07:44:47,771 - SparkDataLoader - INFO - SparkSession inicializada correctamente\n",
      "2025-11-06 07:44:47,773 - SparkDataLoader - INFO - Spark Version: 4.0.1\n",
      "2025-11-06 07:44:47,775 - SparkDataLoader - INFO - Iniciando carga de todas las tablas del esquema estrella\n",
      "2025-11-06 07:44:47,775 - SparkDataLoader - INFO - Ruta base: ./datos\n",
      "2025-11-06 07:44:47,776 - SparkDataLoader - INFO - Prefijo: datos_bi_sucio_bebidas\n",
      "2025-11-06 07:44:47,779 - SparkDataLoader - INFO - Cargando dim_tiempo desde: datos\\datos_bi_sucio_bebidas_dim_tiempo.csv\n",
      "2025-11-06 07:44:49,299 - SparkDataLoader - INFO - Tabla dim_tiempo cargada correctamente\n",
      "2025-11-06 07:44:49,301 - SparkDataLoader - INFO - Cargando dim_tienda desde: datos\\datos_bi_sucio_bebidas_dim_tienda.csv\n",
      "2025-11-06 07:44:49,329 - SparkDataLoader - INFO - Tabla dim_tienda cargada correctamente\n",
      "2025-11-06 07:44:49,331 - SparkDataLoader - INFO - Cargando dim_producto desde: datos\\datos_bi_sucio_bebidas_dim_producto.csv\n",
      "2025-11-06 07:44:49,360 - SparkDataLoader - INFO - Tabla dim_producto cargada correctamente\n",
      "2025-11-06 07:44:49,363 - SparkDataLoader - INFO - Cargando fact_ventas desde: datos\\datos_bi_sucio_bebidas_fact_ventas.csv\n",
      "2025-11-06 07:44:49,391 - SparkDataLoader - INFO - Tabla fact_ventas cargada correctamente\n",
      "2025-11-06 07:44:49,392 - SparkDataLoader - INFO - Carga completada: 4/4 tablas\n",
      "2025-11-06 07:44:49,392 - SparkDataLoader - INFO - \n",
      "============================================================\n",
      "2025-11-06 07:44:49,394 - SparkDataLoader - INFO - RESUMEN DE TABLAS CARGADAS\n",
      "2025-11-06 07:44:49,395 - SparkDataLoader - INFO - ============================================================\n",
      "2025-11-06 07:44:51,914 - SparkDataLoader - INFO -   dim_producto         -> 100 registros\n",
      "2025-11-06 07:44:52,102 - SparkDataLoader - INFO -   dim_tiempo           -> 730 registros\n",
      "2025-11-06 07:44:52,233 - SparkDataLoader - INFO -   dim_tienda           -> 50 registros\n",
      "2025-11-06 07:44:54,105 - SparkDataLoader - INFO -   fact_ventas          -> 40,000,000 registros\n",
      "2025-11-06 07:44:54,106 - SparkDataLoader - INFO - ============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opción 1: Cargar todo automáticamente\n",
    "loader = SparkDataLoader()\n",
    "dirty_data=loader.load_all_tables(base_path=\"./datos\", filename_prefix=\"datos_bi_sucio_bebidas\")\n",
    "loader.show_loaded_tables()\n",
    "# # Opción 2: Cargar tablas individuales\n",
    "# loader.load_table('dim_tiempo', './mi_archivo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1430b0a9-ba40-4134-8cb8-a7bf0787b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipo de dirty_data: <class 'dict'>\n",
      "Es un diccionario con 4 tablas\n",
      "Claves disponibles: ['dim_tiempo', 'dim_tienda', 'dim_producto', 'fact_ventas']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTipo de dirty_data: {type(dirty_data)}\")\n",
    "print(f\"Es un diccionario con {len(dirty_data)} tablas\")\n",
    "print(f\"Claves disponibles: {list(dirty_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357f2daf-1017-4198-83bc-b51082a6fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_tiempo=dirty_data[\"dim_tiempo\"]\n",
    "dim_tienda=dirty_data[\"dim_tienda\"]\n",
    "dim_producto=dirty_data[\"dim_producto\"]\n",
    "facts=dirty_data[\"fact_ventas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7c763-0a1d-45e9-b7f0-99be3f119314",
   "metadata": {},
   "source": [
    "# Identificación y limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b1927-3216-47f5-bde8-0956a6bbe032",
   "metadata": {},
   "source": [
    "## Dimensión Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9274f7c-d9da-44dd-ae9c-39f53e08d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fecha_ID: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- Anio: integer (nullable = true)\n",
      " |-- Mes: integer (nullable = true)\n",
      " |-- Nombre_Mes: string (nullable = true)\n",
      " |-- Numero_Semana: integer (nullable = true)\n",
      " |-- Trimestre: string (nullable = true)\n",
      " |-- Estacionalidad_Factor: double (nullable = true)\n",
      "\n",
      "+--------+----------+----+---+----------+-------------+---------+---------------------+\n",
      "|Fecha_ID|     Fecha|Anio|Mes|Nombre_Mes|Numero_Semana|Trimestre|Estacionalidad_Factor|\n",
      "+--------+----------+----+---+----------+-------------+---------+---------------------+\n",
      "|20230101|2023-01-01|2023|  1|     Enero|           52|   2023Q1|  -0.9981855344718586|\n",
      "|20230102|2023-01-02|2023|  1|     Enero|            1|   2023Q1|  -0.9970011699250151|\n",
      "|20230103|2023-01-03|2023|  1|     Enero|            1|   2023Q1|  -0.9955213724144752|\n",
      "|20230104|2023-01-04|2023|  1|     Enero|            1|   2023Q1|   -0.993746580436178|\n",
      "|20230105|2023-01-05|2023|  1|     Enero|            1|   2023Q1|    -0.99167731989929|\n",
      "+--------+----------+----+---+----------+-------------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "+-------+------------------+------------------+------------------+---------------------+\n",
      "|summary|              Anio|               Mes|     Numero_Semana|Estacionalidad_Factor|\n",
      "+-------+------------------+------------------+------------------+---------------------+\n",
      "|  count|               730|               730|               730|                  730|\n",
      "|   mean|            2023.5| 6.512328767123288|              26.5| -1.21059935287893...|\n",
      "| stddev|0.5003428180045024|3.4483031037100096|15.057389073356886|   0.7075915990571406|\n",
      "|    min|              2023|                 1|                 1|  -0.9999629591162655|\n",
      "|    max|              2024|                12|                52|                  1.0|\n",
      "+-------+------------------+------------------+------------------+---------------------+\n",
      "\n",
      "+--------------+-----------+----------+---------+----------------+-------------------+---------------+---------------------------+\n",
      "|Fecha_ID_nulos|Fecha_nulos|Anio_nulos|Mes_nulos|Nombre_Mes_nulos|Numero_Semana_nulos|Trimestre_nulos|Estacionalidad_Factor_nulos|\n",
      "+--------------+-----------+----------+---------+----------------+-------------------+---------------+---------------------------+\n",
      "|             3|          0|         0|        0|               0|                  0|              0|                          0|\n",
      "+--------------+-----------+----------+---------+----------------+-------------------+---------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identificación Campos\n",
    "dim_tiempo.printSchema()\n",
    "dim_tiempo.show(5)\n",
    "dim_tiempo.select(\"Anio\", \"Mes\", \"Numero_Semana\", \"Estacionalidad_Factor\").describe().show()\n",
    "# Ver valores Nullos por columna\n",
    "dim_tiempo_null = dim_tiempo.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(f\"{c}_nulos\") for c in dim_tiempo.columns\n",
    "])\n",
    "dim_tiempo_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913b7168-285a-40f1-8027-679509e817bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. ANÁLISIS DE Fecha_ID (Tipo Mixto)\n",
      "================================================================================\n",
      "\n",
      "Detección de tipos en Fecha_ID:\n",
      "\n",
      "Distribución de tipos:\n",
      "+------------------+-----+\n",
      "|     Fecha_ID_Tipo|count|\n",
      "+------------------+-----+\n",
      "|Numérico_8_digitos|  723|\n",
      "|       No_numérico|    4|\n",
      "|              NULL|    3|\n",
      "+------------------+-----+\n",
      "\n",
      "\n",
      "Ejemplos por tipo:\n",
      "\n",
      "  Tipo: Numérico_8_digitos\n",
      "+--------+\n",
      "|Fecha_ID|\n",
      "+--------+\n",
      "|20230101|\n",
      "|20230102|\n",
      "|20230103|\n",
      "|20230104|\n",
      "|20230105|\n",
      "+--------+\n",
      "\n",
      "\n",
      "  Tipo: Numérico_otro\n",
      "    (Sin ejemplos)\n",
      "\n",
      "  Tipo: No_numérico\n",
      "+----------------+\n",
      "|Fecha_ID        |\n",
      "+----------------+\n",
      "|2024-X-01       |\n",
      "|2024-X-01       |\n",
      "|ID_ERRONEO      |\n",
      "|FECHA_INCORRECTA|\n",
      "+----------------+\n",
      "\n",
      "\n",
      "  Tipo: NULL\n",
      "+--------+\n",
      "|Fecha_ID|\n",
      "+--------+\n",
      "|NULL    |\n",
      "|NULL    |\n",
      "|NULL    |\n",
      "+--------+\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. ANÁLISIS DE Nombre_Mes (Formato Diferente Nombre Mes)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total de valores distintos: 18\n",
      "Frecuencia de cada variación:\n",
      "+----------+-----+\n",
      "|Nombre_Mes|count|\n",
      "+----------+-----+\n",
      "|ABR.      |1    |\n",
      "|AGO.      |1    |\n",
      "|Abril     |59   |\n",
      "|Agosto    |61   |\n",
      "|DICIEMBRE |1    |\n",
      "|Diciembre |60   |\n",
      "|Enero     |62   |\n",
      "|Febrero   |57   |\n",
      "|Julio     |62   |\n",
      "|Junio     |60   |\n",
      "|MAR.      |2    |\n",
      "|MARZO     |1    |\n",
      "|Marzo     |59   |\n",
      "|Mayo      |62   |\n",
      "|NOV.      |1    |\n",
      "|Noviembre |59   |\n",
      "|Octubre   |62   |\n",
      "|Septiembre|60   |\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.Validacíón e identificación de valores null o erroneos en columna Fecha_ID\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. ANÁLISIS DE Fecha_ID (Tipo Mixto)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detectar tipos de datos\n",
    "print(\"\\nDetección de tipos en Fecha_ID:\")\n",
    "# Clasificar por patrón\n",
    "dim_tiempo_tipos = dim_tiempo.withColumn(\n",
    "    \"Fecha_ID_Tipo\",\n",
    "    F.when(F.col(\"Fecha_ID\").rlike(\"^[0-9]{8}$\"), \"Numérico_8_digitos\")\n",
    "    .when(F.col(\"Fecha_ID\").rlike(\"^[0-9]+$\"), \"Numérico_otro\")\n",
    "    .when(F.col(\"Fecha_ID\").isNull(), \"NULL\")\n",
    "    .otherwise(\"No_numérico\")\n",
    ")\n",
    "\n",
    "print(\"\\nDistribución de tipos:\")\n",
    "dim_tiempo_tipos.groupBy(\"Fecha_ID_Tipo\").count().orderBy(F.col(\"count\").desc()).show()\n",
    "\n",
    "# Ejemplos de cada tipo\n",
    "print(\"\\nEjemplos por tipo:\")\n",
    "for tipo in [\"Numérico_8_digitos\", \"Numérico_otro\", \"No_numérico\", \"NULL\"]:\n",
    "    print(f\"\\n  Tipo: {tipo}\")\n",
    "    ejemplos = dim_tiempo_tipos.filter(F.col(\"Fecha_ID_Tipo\") == tipo).select(\"Fecha_ID\").limit(5)\n",
    "    if ejemplos.count() > 0:\n",
    "        ejemplos.show(truncate=False)\n",
    "    else:\n",
    "        print(\"    (Sin ejemplos)\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------\n",
    "#2.Validacíón e identificación de valores null o erroneos en columna Nombre_Mes\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"2. ANÁLISIS DE Nombre_Mes (Formato Diferente Nombre Mes)\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "# Contar elementos distintos\n",
    "nombre_mes_distinct= dim_tiempo.select(\"Nombre_Mes\").distinct().orderBy(\"Nombre_Mes\").cache()\n",
    "total_distintos= nombre_mes_distinct.count()\n",
    "print(f\"\\nTotal de valores distintos: {total_distintos}\")\n",
    "\n",
    "# Ver con frecuencias \n",
    "print(\"Frecuencia de cada variación:\")\n",
    "dim_tiempo.groupBy(\"Nombre_Mes\").count().orderBy(\"Nombre_Mes\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ac101-8c70-429d-878b-9335e377d2e4",
   "metadata": {},
   "source": [
    "## Dimensión Tienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70addc17-cae3-4b57-8ba8-6fe3b81672ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Tienda_ID: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Formato_Tienda: string (nullable = true)\n",
      " |-- Atractivo_Factor: double (nullable = true)\n",
      "\n",
      "+---------+---------+-------------------+----------------+\n",
      "|Tienda_ID|   Region|     Formato_Tienda|Atractivo_Factor|\n",
      "+---------+---------+-------------------+----------------+\n",
      "|        1|    Norte|       Supermercado|            1.26|\n",
      "|        2|    Norte|       Supermercado|            1.49|\n",
      "|        3|   Centro|       Supermercado|             1.4|\n",
      "|        4|      Sur|       Supermercado|            1.31|\n",
      "|        5|Occidente|Tienda_Conveniencia|             1.4|\n",
      "+---------+---------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "+-------+------------------+------+--------------------+------------------+\n",
      "|summary|         Tienda_ID|Region|      Formato_Tienda|  Atractivo_Factor|\n",
      "+-------+------------------+------+--------------------+------------------+\n",
      "|  count|                50|    50|                  50|                50|\n",
      "|   mean|              25.5|  NULL|                NULL|             0.982|\n",
      "| stddev|14.577379737113251|  NULL|                NULL|0.3125340797743634|\n",
      "|    min|                 1|Centro|        Supermercado|               0.5|\n",
      "|    max|                50|   Sur|Tienda_Especializada|               1.5|\n",
      "+-------+------------------+------+--------------------+------------------+\n",
      "\n",
      "+---------------+------------+--------------------+----------------------+\n",
      "|Tienda_ID_nulos|Region_nulos|Formato_Tienda_nulos|Atractivo_Factor_nulos|\n",
      "+---------------+------------+--------------------+----------------------+\n",
      "|              0|           0|                   0|                     0|\n",
      "+---------------+------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identificación Campos\n",
    "dim_tienda.printSchema()\n",
    "dim_tienda.show(5)\n",
    "dim_tienda.describe().show()\n",
    "# Ver valores Nullos por columna\n",
    "dim_tienda_null = dim_tienda.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(f\"{c}_nulos\") for c in dim_tienda.columns\n",
    "])\n",
    "dim_tienda_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10deac2d-9859-470f-932e-f49c332fddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. ANÁLISIS DE Region (Nombres no Homologados)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total de valores distintos: 4\n",
      "Frecuencia de cada variación:\n",
      "+---------+-----+\n",
      "|Region   |count|\n",
      "+---------+-----+\n",
      "|Centro   |18   |\n",
      "|Norte    |14   |\n",
      "|Occidente|5    |\n",
      "|Sur      |13   |\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. ANÁLISIS DE Formato_Tienda (Nombres no Homologados)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total de valores distintos: 3\n",
      "Frecuencia de cada variación:\n",
      "+--------------------+-----+\n",
      "|Formato_Tienda      |count|\n",
      "+--------------------+-----+\n",
      "|Supermercado        |29   |\n",
      "|Tienda_Conveniencia |11   |\n",
      "|Tienda_Especializada|10   |\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.Validacíón e identificación de valores null o erroneos en columna Región\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"1. ANÁLISIS DE Region (Nombres no Homologados)\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "# Contar elementos distintos\n",
    "nombre_region_distinct= dim_tienda.select(\"Region\").distinct().orderBy(\"Region\").cache()\n",
    "total_distintos= nombre_region_distinct.count()\n",
    "print(f\"\\nTotal de valores distintos: {total_distintos}\")\n",
    "\n",
    "# Ver con frecuencias \n",
    "print(\"Frecuencia de cada variación:\")\n",
    "dim_tienda.groupBy(\"Region\").count().orderBy(\"Region\").show(30, truncate=False)\n",
    "\n",
    "#2.Validacíón e identificación de valores null o erroneos en columna Formato_Tienda\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"2. ANÁLISIS DE Formato_Tienda (Nombres no Homologados)\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "# Contar elementos distintos \n",
    "nombre_region_distinct= dim_tienda.select(\"Formato_Tienda\").distinct().orderBy(\"Formato_Tienda\").cache()\n",
    "total_distintos= nombre_region_distinct.count()\n",
    "print(f\"\\nTotal de valores distintos: {total_distintos}\")\n",
    "\n",
    "# Ver con frecuencias \n",
    "print(\"Frecuencia de cada variación:\")\n",
    "dim_tienda.groupBy(\"Formato_Tienda\").count().orderBy(\"Formato_Tienda\").show(30, truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4680df-e3c2-412a-9853-bb58d0cce86d",
   "metadata": {},
   "source": [
    "## Dimensión Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e01bb27-3460-45b2-8d9c-116fbe7f61eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Tienda_ID: integer (nullable = true)\n",
      " |-- Producto_ID: integer (nullable = true)\n",
      " |-- Fecha_ID: string (nullable = true)\n",
      " |-- Distribucion_Numerica: double (nullable = true)\n",
      " |-- Distribucion_Ponderada: double (nullable = true)\n",
      " |-- Precio: double (nullable = true)\n",
      " |-- Out_Of_Stock_Flag: integer (nullable = true)\n",
      " |-- Ventas_Volumen: integer (nullable = true)\n",
      " |-- Ventas_Valor: double (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+------------------+---------------------+----------------------+\n",
      "|summary|         Tienda_ID|       Producto_ID|          Fecha_ID|Distribucion_Numerica|Distribucion_Ponderada|\n",
      "+-------+------------------+------------------+------------------+---------------------+----------------------+\n",
      "|  count|          40000000|          40000000|          40000000|             40000000|              40000000|\n",
      "|   mean|        25.5021886|     1050.50450245|2.02356462636302E7|   0.7499863614999879|    0.7999894627439181|\n",
      "| stddev|14.430535728656716|28.866788112123547|  5010.07464867386|  0.14438839321493682|   0.11554184439074092|\n",
      "|    min|                 1|              1001|          20230101|                  0.5|                   0.6|\n",
      "|    max|                50|              1100|          20241230|                  1.0|                   1.0|\n",
      "+-------+------------------+------------------+------------------+---------------------+----------------------+\n",
      "\n",
      "+-------+-----------------+------------------+-----------------+--------------------+\n",
      "|summary|           Precio| Out_Of_Stock_Flag|   Ventas_Volumen|        Ventas_Valor|\n",
      "+-------+-----------------+------------------+-----------------+--------------------+\n",
      "|  count|         39600000|          40000000|         40000000|            40000000|\n",
      "|   mean|2.525681107070904|       0.050011975|     -0.042974775|-0.11172990649999724|\n",
      "| stddev|1.346553920144291|0.2179696734510785|73.86490860073103|  212.62445888350652|\n",
      "|    min|             0.45|                 0|             -213|            -1121.48|\n",
      "|    max|             5.41|                 1|              214|             1126.77|\n",
      "+-------+-----------------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identificación Campos\n",
    "facts.printSchema()\n",
    "# facts.show(5)\n",
    "facts.describe(\"Tienda_ID\", \"Producto_ID\", \"Fecha_ID\", \"Distribucion_Numerica\", \"Distribucion_Ponderada\").show()\n",
    "facts.describe(\"Precio\", \"Out_Of_Stock_Flag\", \"Ventas_Volumen\", \"Ventas_Valor\").show()\n",
    "# facts.describe().show()\n",
    "# Ver valores Nullos por columna\n",
    "facts_null = facts.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(f\"{c}_nulos\") for c in facts.columns\n",
    "])\n",
    "\n",
    "\n",
    "# facts_null.select(\"Fecha_ID\", \"Precio\").show()\n",
    "# facts_null.select(\"Precio\", \"Out_Of_Stock_Flag\", \"Ventas_Volumen\", \"Ventas_Valor\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c70aa995-6d1b-46d1-bf14-eca15587b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. ANÁLISIS DE Fecha_ID\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Distribución de tipos:\n",
      "+------------------+--------+\n",
      "|     Fecha_ID_Tipo|   count|\n",
      "+------------------+--------+\n",
      "|Numérico_8_digitos|40000000|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.Validacíón e identificación de valores null o erroneos en columna Fecha_ID\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"1. ANÁLISIS DE Fecha_ID\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "# Clasificar por patrón\n",
    "facts_fecha_id_tipos = facts.withColumn(\n",
    "    \"Fecha_ID_Tipo\",\n",
    "    F.when(F.col(\"Fecha_ID\").rlike(\"^[0-9]{8}$\"), \"Numérico_8_digitos\")\n",
    "    .when(F.col(\"Fecha_ID\").rlike(\"^[0-9]+$\"), \"Numérico_otro\")\n",
    "    .when(F.col(\"Fecha_ID\").isNull(), \"NULL\")\n",
    "    .otherwise(\"No_numérico\")\n",
    ")\n",
    "\n",
    "print(\"\\nDistribución de tipos:\")\n",
    "facts_fecha_id_tipos.groupBy(\"Fecha_ID_Tipo\").count().orderBy(F.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a0e0490-3ef5-4f28-b991-198c5678705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. Venta_Normal\n",
      "--------------------------------------------------------------------------------\n",
      " % Vtas Normales : 45.87%\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|Tienda_ID|Producto_ID|Fecha_ID|Distribucion_Numerica|Distribucion_Ponderada|Precio|Out_Of_Stock_Flag|Ventas_Volumen|Ventas_Valor|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|       30|       1055|20240822|                 0.62|                  0.73|  1.04|                0|            25|        26.0|\n",
      "|       47|       1089|20230524|                  0.6|                  0.63|  4.67|                0|            99|      462.33|\n",
      "|       20|       1014|20230809|                 0.76|                  0.68|  1.04|                0|            58|       60.32|\n",
      "|       20|       1092|20240617|                 0.86|                  0.78|  0.88|                0|           182|      160.16|\n",
      "|       48|       1004|20230817|                 0.62|                  0.94|  2.67|                0|            43|      114.81|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. Devolución\n",
      "--------------------------------------------------------------------------------\n",
      " % Devolución : 48.26% (vts Vol < 0 & Precio > 0)\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|Tienda_ID|Producto_ID|Fecha_ID|Distribucion_Numerica|Distribucion_Ponderada|Precio|Out_Of_Stock_Flag|Ventas_Volumen|Ventas_Valor|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|       24|       1008|20231202|                 0.78|                  0.68|   1.1|                0|          -134|      -147.4|\n",
      "|       41|       1008|20241115|                 0.75|                  0.87|  0.99|                0|           -66|      -65.34|\n",
      "|       23|       1003|20230220|                 0.76|                  0.72|  4.99|                0|           -44|     -219.56|\n",
      "|       47|       1080|20230210|                  1.0|                  0.61|  0.94|                0|           -51|      -47.94|\n",
      "|       41|       1038|20230124|                 0.99|                  0.85|  2.82|                0|           -57|     -160.74|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3. Ajuste de Inventario\n",
      "--------------------------------------------------------------------------------\n",
      " % Ajuste de Inventario : 2.41% (vts Vol <0 & Precio > 0 OOS =1)\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|Tienda_ID|Producto_ID|Fecha_ID|Distribucion_Numerica|Distribucion_Ponderada|Precio|Out_Of_Stock_Flag|Ventas_Volumen|Ventas_Valor|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|       13|       1043|20240110|                 0.77|                  0.77|  1.96|                1|           -51|      -99.96|\n",
      "|       33|       1049|20230108|                  0.5|                  0.94|  4.48|                1|           -94|     -421.12|\n",
      "|       25|       1021|20231109|                 0.66|                  0.77|   4.1|                1|           -96|      -393.6|\n",
      "|       12|       1036|20231108|                 0.82|                  0.94|  1.36|                1|          -116|     -157.76|\n",
      "|        7|       1014|20231109|                 0.94|                   0.9|  1.16|                1|           -68|      -78.88|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4. Venta Perdida\n",
      "--------------------------------------------------------------------------------\n",
      " % Venta Perdida : 0.07% (vts Vol=0 & OOS=1 & vts Val =0 | vts Val = null)\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|Tienda_ID|Producto_ID|Fecha_ID|Distribucion_Numerica|Distribucion_Ponderada|Precio|Out_Of_Stock_Flag|Ventas_Volumen|Ventas_Valor|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|       18|       1012|20240901|                 0.72|                  0.83|   3.5|                1|             0|         0.0|\n",
      "|        5|       1041|20240301|                 0.93|                  0.66|  0.53|                1|             0|         0.0|\n",
      "|        3|       1015|20240302|                 0.79|                  0.67|  4.35|                1|             0|         0.0|\n",
      "|       45|       1012|20240303|                 0.98|                   0.9|  3.49|                1|             0|         0.0|\n",
      "|        9|       1057|20240902|                 0.98|                  0.75|  4.75|                1|             0|         0.0|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5. Registro sin venta\n",
      "--------------------------------------------------------------------------------\n",
      " % Registro sin Venta : 1.41% (vts Vol=0 & OOS=0 & vts Val =0 | vts Val = null)\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|Tienda_ID|Producto_ID|Fecha_ID|Distribucion_Numerica|Distribucion_Ponderada|Precio|Out_Of_Stock_Flag|Ventas_Volumen|Ventas_Valor|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "|       18|       1012|20240901|                 0.72|                  0.83|   3.5|                1|             0|         0.0|\n",
      "|        5|       1041|20240301|                 0.93|                  0.66|  0.53|                1|             0|         0.0|\n",
      "|        3|       1015|20240302|                 0.79|                  0.67|  4.35|                1|             0|         0.0|\n",
      "|       45|       1012|20240303|                 0.98|                   0.9|  3.49|                1|             0|         0.0|\n",
      "|        9|       1057|20240902|                 0.98|                  0.75|  4.75|                1|             0|         0.0|\n",
      "+---------+-----------+--------+---------------------+----------------------+------+-----------------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "Total Escencarios 98.038768%\n"
     ]
    }
   ],
   "source": [
    "#2.Validacíón e identificación de valores variables numéricas\n",
    "#-------------------------------Venta_Normal\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"1. Venta_Normal\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "total_registros = facts.count()\n",
    "vta_normal=facts.filter(\n",
    "    (F.col(\"Ventas_Valor\")>0) & \n",
    "    (F.col(\"Ventas_Volumen\")>0) & \n",
    "    (F.col(\"Precio\")>0) &\n",
    "    (F.col(\"Out_Of_Stock_Flag\") ==0)\n",
    "          )\n",
    "porcentaje_vts_normal=vta_normal.count()/ total_registros\n",
    "\n",
    "print(f\" % Vtas Normales : {porcentaje_vts_normal:.2%}\")\n",
    "vta_normal.show(5)\n",
    "\n",
    "#-------------------------------Devolución\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"2. Devolución\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "devolucion=facts.filter(\n",
    "    (F.col(\"Ventas_Volumen\")<0) &\n",
    "    (F.col(\"Precio\")>0)\n",
    "          )\n",
    "porcentaje_devolucion=devolucion.count()/ total_registros\n",
    "\n",
    "print(f\" % Devolución : {porcentaje_devolucion:.2%} (vts Vol < 0 & Precio > 0)\")\n",
    "devolucion.show(5)\n",
    "\n",
    "#-------------------------------Ajuste de Inventario\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"3. Ajuste de Inventario\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "ajuste_de_inventario=facts.filter(\n",
    "    (F.col(\"Ventas_Volumen\")<0) &\n",
    "    (F.col(\"Precio\")>0) &\n",
    "    (F.col(\"Out_Of_Stock_Flag\") ==1)\n",
    "          )\n",
    "porcentaje_ajuste_de_inventario=ajuste_de_inventario.count()/ total_registros\n",
    "\n",
    "print(f\" % Ajuste de Inventario : {porcentaje_ajuste_de_inventario:.2%} (vts Vol <0 & Precio > 0 OOS =1)\")\n",
    "ajuste_de_inventario.show(5)\n",
    "\n",
    "#-------------------------------Venta perdida\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"4. Venta Perdida\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "venta_perdida=facts.filter(\n",
    "    (F.col(\"Ventas_Volumen\")==0) &\n",
    "    (F.col(\"Out_Of_Stock_Flag\") ==1) &\n",
    "    (F.col(\"Ventas_Valor\")==0) | (F.col(\"Ventas_Valor\").isNull())\n",
    "          )\n",
    "porcentaje_venta_perdida=venta_perdida.count()/ total_registros\n",
    "\n",
    "print(f\" % Venta Perdida : {porcentaje_venta_perdida:.2%} (vts Vol=0 & OOS=1 & vts Val =0 | vts Val = null)\")\n",
    "venta_perdida.show(5)\n",
    "\n",
    "#-------------------------------Registro sin Venta\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"5. Registro sin venta\")\n",
    "print(f\"{'-'*80}\")\n",
    "\n",
    "registro_sin_venta=facts.filter(\n",
    "    (F.col(\"Ventas_Volumen\")==0) &\n",
    "    (F.col(\"Out_Of_Stock_Flag\") ==0) &\n",
    "    (F.col(\"Ventas_Valor\")==0) | (F.col(\"Ventas_Valor\").isNull())\n",
    "          )\n",
    "porcentaje_registro_sin_venta=registro_sin_venta.count()/ total_registros\n",
    "\n",
    "print(f\" % Registro sin Venta : {porcentaje_registro_sin_venta:.2%} (vts Vol=0 & OOS=0 & vts Val =0 | vts Val = null)\")\n",
    "venta_perdida.show(5)\n",
    "\n",
    "Total_errores= porcentaje_vts_normal + porcentaje_devolucion + porcentaje_ajuste_de_inventario + porcentaje_venta_perdida +  porcentaje_registro_sin_venta\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total Escencarios {Total_errores:2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210febbe-beae-479c-9812-4be392ef2d57",
   "metadata": {},
   "source": [
    "# Proceso de Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f16e5f-94c6-4802-a76d-57f2b29075f8",
   "metadata": {},
   "source": [
    "## Escenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45281825-7796-40c5-bc9a-0a7fe98b4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se inicia la función para limpiar el conjunto de datos\n",
    "cleaner=SparkDataCleaner(dataframes=dirty_data)\n",
    "dim_tiempo= cleaner.clean_dim_tiempo()\n",
    "dim_tienda=cleaner.clean_dim_tienda()\n",
    "facts = cleaner.clean_fact_ventas(\n",
    "    remove_null_precio=False,\n",
    "    remove_null_volumen=True,\n",
    "    impute_distribucion=True,\n",
    "    recalculate_valor=True,\n",
    "    remove_outliers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e7c8e-6195-4eec-88dd-e79c76051ab7",
   "metadata": {},
   "source": [
    "# Pipeline (Exportación de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645060e9-8d4b-4f6b-bae1-5358fda632bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:13:02,773 - SparkDataLoader - INFO - Inicializando SparkSession: PipelineCompleto\n",
      "2025-11-03 21:13:02,950 - SparkDataLoader - INFO - SparkSession inicializada correctamente\n",
      "2025-11-03 21:13:02,951 - SparkDataLoader - INFO - Spark Version: 4.0.1\n",
      "2025-11-03 21:13:02,952 - SparkDataLoader - INFO - Iniciando carga de todas las tablas del esquema estrella\n",
      "2025-11-03 21:13:02,953 - SparkDataLoader - INFO - Ruta base: ./datos\n",
      "2025-11-03 21:13:02,953 - SparkDataLoader - INFO - Prefijo: datos_bi_sucio_bebidas\n",
      "2025-11-03 21:13:02,957 - SparkDataLoader - INFO - Cargando dim_tiempo desde: datos\\datos_bi_sucio_bebidas_dim_tiempo.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE: CARGA → LIMPIEZA → EXPORTACIÓN\n",
      "================================================================================\n",
      "\n",
      "[1/4] Cargando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:13:03,141 - SparkDataLoader - INFO - Tabla dim_tiempo cargada correctamente\n",
      "2025-11-03 21:13:03,144 - SparkDataLoader - INFO - Cargando dim_tienda desde: datos\\datos_bi_sucio_bebidas_dim_tienda.csv\n",
      "2025-11-03 21:13:03,160 - SparkDataLoader - INFO - Tabla dim_tienda cargada correctamente\n",
      "2025-11-03 21:13:03,164 - SparkDataLoader - INFO - Cargando dim_producto desde: datos\\datos_bi_sucio_bebidas_dim_producto.csv\n",
      "2025-11-03 21:13:03,182 - SparkDataLoader - INFO - Tabla dim_producto cargada correctamente\n",
      "2025-11-03 21:13:03,185 - SparkDataLoader - INFO - Cargando fact_ventas desde: datos\\datos_bi_sucio_bebidas_fact_ventas.csv\n",
      "2025-11-03 21:13:03,204 - SparkDataLoader - INFO - Tabla fact_ventas cargada correctamente\n",
      "2025-11-03 21:13:03,204 - SparkDataLoader - INFO - Carga completada: 4/4 tablas\n",
      "2025-11-03 21:13:03,205 - SparkDataLoader - INFO - \n",
      "============================================================\n",
      "2025-11-03 21:13:03,206 - SparkDataLoader - INFO - RESUMEN DE TABLAS CARGADAS\n",
      "2025-11-03 21:13:03,206 - SparkDataLoader - INFO - ============================================================\n",
      "2025-11-03 21:13:03,638 - SparkDataLoader - INFO -   dim_producto         -> 100 registros\n",
      "2025-11-03 21:13:03,764 - SparkDataLoader - INFO -   dim_tiempo           -> 730 registros\n",
      "2025-11-03 21:13:03,857 - SparkDataLoader - INFO -   dim_tienda           -> 50 registros\n",
      "2025-11-03 21:13:06,477 - SparkDataLoader - INFO -   fact_ventas          -> 40,000,000 registros\n",
      "2025-11-03 21:13:06,478 - SparkDataLoader - INFO - ============================================================\n",
      "\n",
      "2025-11-03 21:13:06,479 - SparkDataCleaner - INFO - DataCleaner inicializado\n",
      "2025-11-03 21:13:06,480 - SparkDataCleaner - INFO - Tablas recibidas: ['dim_tiempo', 'dim_tienda', 'dim_producto', 'fact_ventas']\n",
      "2025-11-03 21:13:06,481 - SparkDataCleaner - INFO - \n",
      "======================================================================\n",
      "2025-11-03 21:13:06,483 - SparkDataCleaner - INFO -                     INICIANDO LIMPIEZA COMPLETA\n",
      "2025-11-03 21:13:06,484 - SparkDataCleaner - INFO - ======================================================================\n",
      "2025-11-03 21:13:06,484 - SparkDataCleaner - INFO - \n",
      "============================================================\n",
      "2025-11-03 21:13:06,486 - SparkDataCleaner - INFO - LIMPIANDO: dim_tiempo\n",
      "2025-11-03 21:13:06,486 - SparkDataCleaner - INFO - ============================================================\n",
      "2025-11-03 21:13:06,562 - SparkDataCleaner - INFO - Registros iniciales: 730\n",
      "2025-11-03 21:13:06,563 - SparkDataCleaner - INFO - Convirtiendo columna Fecha a DateType (formato: yyyy-MM-dd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:13:06,794 - SparkDataCleaner - INFO - Fechas convertidas exitosamente: 730 (100.00%)\n",
      "2025-11-03 21:13:06,903 - SparkDataCleaner - INFO - Fecha_IDs inválidos encontrados: 4\n",
      "2025-11-03 21:13:06,904 - SparkDataCleaner - WARNING - Mostrando ejemplos de Fecha_IDs inválidos:\n",
      "2025-11-03 21:13:06,999 - SparkDataCleaner - INFO - Intentando corregir Fecha_IDs inválidos usando columna Fecha...\n",
      "2025-11-03 21:13:07,172 - SparkDataCleaner - INFO - Fecha_IDs corregidos desde columna Fecha: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|Fecha_ID        |Fecha     |\n",
      "+----------------+----------+\n",
      "|2024-X-01       |2023-01-12|\n",
      "|2024-X-01       |2023-06-02|\n",
      "|ID_ERRONEO      |2024-05-13|\n",
      "|FECHA_INCORRECTA|2024-06-06|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:13:08,050 - SparkDataCleaner - INFO - Registros finales: 730\n",
      "2025-11-03 21:13:08,051 - SparkDataCleaner - INFO - Registros eliminados: 0 (0.00%)\n",
      "2025-11-03 21:13:08,052 - SparkDataCleaner - INFO - \n",
      "============================================================\n",
      "2025-11-03 21:13:08,053 - SparkDataCleaner - INFO - LIMPIANDO: dim_tienda\n",
      "2025-11-03 21:13:08,053 - SparkDataCleaner - INFO - ============================================================\n",
      "2025-11-03 21:13:08,111 - SparkDataCleaner - INFO - Registros iniciales: 50\n",
      "2025-11-03 21:13:08,512 - SparkDataCleaner - INFO - Registros finales: 50\n",
      "2025-11-03 21:13:08,515 - SparkDataCleaner - INFO - Registros eliminados: 0 (0.00%)\n",
      "2025-11-03 21:13:08,516 - SparkDataCleaner - INFO - \n",
      "============================================================\n",
      "2025-11-03 21:13:08,518 - SparkDataCleaner - INFO - LIMPIANDO: dim_producto\n",
      "2025-11-03 21:13:08,522 - SparkDataCleaner - INFO - ============================================================\n",
      "2025-11-03 21:13:08,626 - SparkDataCleaner - INFO - Registros iniciales: 100\n",
      "2025-11-03 21:13:09,037 - SparkDataCleaner - INFO - Registros finales: 100\n",
      "2025-11-03 21:13:09,038 - SparkDataCleaner - INFO - Registros eliminados: 0 (0.00%)\n",
      "2025-11-03 21:13:09,039 - SparkDataCleaner - INFO - \n",
      "============================================================\n",
      "2025-11-03 21:13:09,040 - SparkDataCleaner - INFO - LIMPIANDO: fact_ventas\n",
      "2025-11-03 21:13:09,040 - SparkDataCleaner - INFO - ============================================================\n",
      "2025-11-03 21:13:10,470 - SparkDataCleaner - INFO - Registros iniciales: 40,000,000\n",
      "2025-11-03 21:13:10,471 - SparkDataCleaner - INFO - \n",
      "--- PASO 1: Limpieza de Fecha_ID ---\n",
      "2025-11-03 21:13:17,627 - SparkDataCleaner - INFO - Fecha_IDs inválidos encontrados: 0\n",
      "2025-11-03 21:13:19,146 - SparkDataCleaner - INFO - Registros después de limpieza Fecha_ID: 40,000,000 (sin eliminaciones)\n",
      "2025-11-03 21:13:19,146 - SparkDataCleaner - INFO - \n",
      "--- PASO 2: Calcular Precio Promedio por Producto ---\n",
      "2025-11-03 21:13:19,185 - SparkDataCleaner - INFO - ✓ Precio promedio por producto calculado\n",
      "2025-11-03 21:13:37,230 - SparkDataCleaner - INFO - \n",
      "--- PASO 3: Clasificar Transacciones ---\n",
      "2025-11-03 21:13:37,382 - SparkDataCleaner - INFO - Distribución de tipos de transacción:\n",
      "2025-11-03 21:13:48,032 - SparkDataCleaner - INFO - \n",
      "--- PASO 4: Limpieza y Corrección de Precio ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|Tipo_Transaccion  |count   |\n",
      "+------------------+--------+\n",
      "|VENTA_NORMAL      |19315650|\n",
      "|DEVOLUCION        |19305125|\n",
      "|ANOMALO           |589257  |\n",
      "|REGISTRO_SIN_VENTA|565055  |\n",
      "|AJUSTE_INVENTARIO |195049  |\n",
      "|VENTA_PERDIDA     |29864   |\n",
      "+------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:13:54,722 - SparkDataCleaner - INFO - Registros con Precio NULL: 400,000\n",
      "2025-11-03 21:13:54,734 - SparkDataCleaner - INFO - ✓ Precio NULL imputado con precio promedio del producto\n",
      "2025-11-03 21:14:01,278 - SparkDataCleaner - INFO - Registros con Precio <= 0 y Volumen > 0: 0\n",
      "2025-11-03 21:14:20,417 - SparkDataCleaner - INFO - \n",
      "--- PASO 5: Limpieza de Ventas_Volumen ---\n",
      "2025-11-03 21:14:26,678 - SparkDataCleaner - INFO - Registros con Ventas_Volumen NULL: 0\n",
      "2025-11-03 21:14:26,679 - SparkDataCleaner - INFO - \n",
      "--- PASO 6: Recalcular Ventas_Valor ---\n",
      "2025-11-03 21:14:26,703 - SparkDataCleaner - INFO - ✓ Ventas_Valor recalculado como Precio * Ventas_Volumen\n",
      "2025-11-03 21:14:49,365 - SparkDataCleaner - WARNING - Discrepancias en Ventas_Valor corregidas: 745,893\n",
      "2025-11-03 21:14:49,366 - SparkDataCleaner - INFO - \n",
      "--- PASO 7: Manejo de Registros Anómalos ---\n",
      "2025-11-03 21:15:00,337 - SparkDataCleaner - INFO - Registros anómalos identificados: 589,257\n",
      "2025-11-03 21:15:00,337 - SparkDataCleaner - WARNING - Ejemplos de registros anómalos:\n",
      "2025-11-03 21:15:19,698 - SparkDataCleaner - INFO - Los registros anómalos se mantienen marcados con Es_Anomalo=True\n",
      "2025-11-03 21:15:19,699 - SparkDataCleaner - INFO - Recomendación: Revisar manualmente o excluir de análisis críticos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+---------------+------------------+---------------------+------------------+-----------------+\n",
      "|Producto_ID|Ventas_Volumen|Precio_Original|Precio            |Ventas_Valor_Original|Ventas_Valor      |Out_Of_Stock_Flag|\n",
      "+-----------+--------------+---------------+------------------+---------------------+------------------+-----------------+\n",
      "|1088       |0             |1.74           |1.74              |-107.88              |0.0               |0                |\n",
      "|1088       |0             |2.01           |2.01              |-100.5               |0.0               |1                |\n",
      "|1088       |0             |1.78           |1.78              |113.92               |0.0               |0                |\n",
      "|1088       |0             |1.89           |1.89              |-34.02               |0.0               |0                |\n",
      "|1088       |0             |1.9            |1.9               |201.4                |0.0               |0                |\n",
      "|1088       |0             |1.81           |1.81              |126.7                |0.0               |0                |\n",
      "|1088       |0             |1.97           |1.97              |23.64                |0.0               |0                |\n",
      "|1088       |96            |NULL           |1.8600385165681175|190.08               |178.56369759053928|0                |\n",
      "|1088       |0             |1.75           |1.75              |-201.25              |0.0               |0                |\n",
      "|1088       |0             |1.71           |1.71              |179.55               |0.0               |0                |\n",
      "+-----------+--------------+---------------+------------------+---------------------+------------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:16:01,692 - SparkDataCleaner - INFO - Precios <= 0: 0\n",
      "2025-11-03 21:16:01,693 - SparkDataCleaner - INFO - Precios nulos: 0\n",
      "2025-11-03 21:16:08,034 - SparkDataCleaner - INFO - Volúmenes nulos: 0\n",
      "2025-11-03 21:16:08,042 - SparkDataCleaner - INFO - \n",
      "--- PASO 8: Imputar Distribución Ponderada ---\n",
      "2025-11-03 21:16:14,521 - SparkDataCleaner - INFO - Distribucion_Ponderada nulos: 0\n",
      "2025-11-03 21:16:14,522 - SparkDataCleaner - INFO - \n",
      "--- PASO 9: Detección y Eliminación de Outliers ---\n",
      "2025-11-03 21:16:22,240 - SparkDataCleaner - INFO - Registros finales: 40,000,000\n",
      "2025-11-03 21:16:22,240 - SparkDataCleaner - INFO - Registros eliminados: 0 (0.00%)\n",
      "2025-11-03 21:16:22,241 - SparkDataCleaner - INFO - \n",
      "Distribución FINAL por tipo de transacción:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|Tipo_Transaccion  |count   |\n",
      "+------------------+--------+\n",
      "|VENTA_NORMAL      |19315650|\n",
      "|DEVOLUCION        |19305125|\n",
      "|ANOMALO           |589257  |\n",
      "|REGISTRO_SIN_VENTA|565055  |\n",
      "|AJUSTE_INVENTARIO |195049  |\n",
      "|VENTA_PERDIDA     |29864   |\n",
      "+------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:16:46,678 - SparkDataCleaner - INFO - \n",
      "============================================================\n",
      "2025-11-03 21:16:46,679 - SparkDataCleaner - INFO - APLICANDO INTEGRIDAD REFERENCIAL\n",
      "2025-11-03 21:16:46,681 - SparkDataCleaner - INFO - ============================================================\n",
      "2025-11-03 21:17:04,347 - SparkDataCleaner - INFO - Registros con Fecha_ID huérfano eliminados: 0\n",
      "2025-11-03 21:17:14,928 - SparkDataCleaner - INFO - Registros con Tienda_ID huérfano eliminados: 0\n",
      "2025-11-03 21:17:25,546 - SparkDataCleaner - INFO - Registros con Producto_ID huérfano eliminados: 0\n",
      "2025-11-03 21:17:25,547 - SparkDataCleaner - INFO - \n",
      "Total registros eliminados: 0 (0.00%)\n",
      "2025-11-03 21:17:25,549 - SparkDataCleaner - INFO - Registros finales: 40,000,000\n",
      "2025-11-03 21:17:25,549 - SparkDataCleaner - INFO - \n",
      "======================================================================\n",
      "2025-11-03 21:17:25,550 - SparkDataCleaner - INFO -                     LIMPIEZA COMPLETADA\n",
      "2025-11-03 21:17:25,552 - SparkDataCleaner - INFO - ======================================================================\n",
      "2025-11-03 21:17:25,555 - SparkDataExporter - INFO - ✓ Configuración SQL Server inicializada\n",
      "2025-11-03 21:17:25,556 - SparkDataExporter - INFO -   Driver class: com.microsoft.sqlserver.jdbc.SQLServerDriver\n",
      "2025-11-03 21:17:25,559 - SparkDataExporter - INFO - \n",
      "======================================================================\n",
      "2025-11-03 21:17:25,560 - SparkDataExporter - INFO - EXPORTACIÓN MASIVA: 4 tablas a SQL Server\n",
      "2025-11-03 21:17:25,561 - SparkDataExporter - INFO - ======================================================================\n",
      "2025-11-03 21:17:25,561 - SparkDataExporter - INFO - \n",
      "--- Exportando: clean_dim_tiempo ---\n",
      "2025-11-03 21:17:25,562 - SparkDataExporter - INFO - ⏳ Iniciando: export_to_sql\n",
      "2025-11-03 21:17:25,633 - SparkDataExporter - INFO - 📊 Total de registros a exportar: 730\n",
      "2025-11-03 21:17:25,634 - SparkDataExporter - INFO - 🔧 Reparticionando en 4 particiones...\n",
      "2025-11-03 21:17:25,637 - SparkDataExporter - INFO - 🚀 Intento 1/3: Exportando a SQL Server → clean_dim_tiempo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    RESUMEN DE LIMPIEZA\n",
      "======================================================================\n",
      "\n",
      "DIM_TIEMPO\n",
      "------------------------------------------------------------\n",
      "  Registros iniciales:           730\n",
      "  Registros finales:             730\n",
      "  Registros eliminados:            0\n",
      "  Porcentaje eliminado:         0.00%\n",
      "\n",
      "DIM_TIENDA\n",
      "------------------------------------------------------------\n",
      "  Registros iniciales:            50\n",
      "  Registros finales:              50\n",
      "  Registros eliminados:            0\n",
      "  Porcentaje eliminado:         0.00%\n",
      "\n",
      "DIM_PRODUCTO\n",
      "------------------------------------------------------------\n",
      "  Registros iniciales:           100\n",
      "  Registros finales:             100\n",
      "  Registros eliminados:            0\n",
      "  Porcentaje eliminado:         0.00%\n",
      "\n",
      "FACT_VENTAS\n",
      "------------------------------------------------------------\n",
      "  Registros iniciales:    40,000,000\n",
      "  Registros finales:      40,000,000\n",
      "  Registros eliminados:            0\n",
      "  Porcentaje eliminado:         0.00%\n",
      "\n",
      "  Integridad Referencial:\n",
      "    - Fecha_ID huérfanos:             0\n",
      "    - Tienda_ID huérfanos:            0\n",
      "    - Producto_ID huérfanos:          0\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[3/4] Exportando a SQL Server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:17:26,307 - SparkDataExporter - INFO - ✅✅✅ EXPORTACIÓN EXITOSA A SQL SERVER ✅✅✅\n",
      "2025-11-03 21:17:26,308 - SparkDataExporter - INFO - 📊 Total exportado: 730 registros\n",
      "2025-11-03 21:17:26,309 - SparkDataExporter - INFO - 📍 Tabla destino: clean_dim_tiempo\n",
      "2025-11-03 21:17:26,310 - SparkDataExporter - INFO - ⚙️ Modo: overwrite\n",
      "2025-11-03 21:17:26,311 - SparkDataExporter - INFO - ⏱️ Tiempo total en export_to_sql: 0.75 segundos (0.01 minutos)\n",
      "2025-11-03 21:17:26,312 - SparkDataExporter - INFO - \n",
      "--- Exportando: clean_dim_tienda ---\n",
      "2025-11-03 21:17:26,312 - SparkDataExporter - INFO - ⏳ Iniciando: export_to_sql\n",
      "2025-11-03 21:17:26,374 - SparkDataExporter - INFO - 📊 Total de registros a exportar: 50\n",
      "2025-11-03 21:17:26,375 - SparkDataExporter - INFO - 🔧 Reparticionando en 4 particiones...\n",
      "2025-11-03 21:17:26,378 - SparkDataExporter - INFO - 🚀 Intento 1/3: Exportando a SQL Server → clean_dim_tienda\n",
      "2025-11-03 21:17:26,622 - SparkDataExporter - INFO - ✅✅✅ EXPORTACIÓN EXITOSA A SQL SERVER ✅✅✅\n",
      "2025-11-03 21:17:26,623 - SparkDataExporter - INFO - 📊 Total exportado: 50 registros\n",
      "2025-11-03 21:17:26,624 - SparkDataExporter - INFO - 📍 Tabla destino: clean_dim_tienda\n",
      "2025-11-03 21:17:26,625 - SparkDataExporter - INFO - ⚙️ Modo: overwrite\n",
      "2025-11-03 21:17:26,626 - SparkDataExporter - INFO - ⏱️ Tiempo total en export_to_sql: 0.31 segundos (0.01 minutos)\n",
      "2025-11-03 21:17:26,628 - SparkDataExporter - INFO - \n",
      "--- Exportando: clean_dim_producto ---\n",
      "2025-11-03 21:17:26,628 - SparkDataExporter - INFO - ⏳ Iniciando: export_to_sql\n",
      "2025-11-03 21:17:26,714 - SparkDataExporter - INFO - 📊 Total de registros a exportar: 100\n",
      "2025-11-03 21:17:26,715 - SparkDataExporter - INFO - 🔧 Reparticionando en 4 particiones...\n",
      "2025-11-03 21:17:26,718 - SparkDataExporter - INFO - 🚀 Intento 1/3: Exportando a SQL Server → clean_dim_producto\n",
      "2025-11-03 21:17:26,946 - SparkDataExporter - INFO - ✅✅✅ EXPORTACIÓN EXITOSA A SQL SERVER ✅✅✅\n",
      "2025-11-03 21:17:26,947 - SparkDataExporter - INFO - 📊 Total exportado: 100 registros\n",
      "2025-11-03 21:17:26,948 - SparkDataExporter - INFO - 📍 Tabla destino: clean_dim_producto\n",
      "2025-11-03 21:17:26,950 - SparkDataExporter - INFO - ⚙️ Modo: overwrite\n",
      "2025-11-03 21:17:26,951 - SparkDataExporter - INFO - ⏱️ Tiempo total en export_to_sql: 0.32 segundos (0.01 minutos)\n",
      "2025-11-03 21:17:26,953 - SparkDataExporter - INFO - \n",
      "--- Exportando: clean_fact_ventas ---\n",
      "2025-11-03 21:17:26,954 - SparkDataExporter - INFO - ⏳ Iniciando: export_to_sql\n",
      "2025-11-03 21:17:38,459 - SparkDataExporter - INFO - 📊 Total de registros a exportar: 40,000,000\n",
      "2025-11-03 21:17:38,460 - SparkDataExporter - INFO - 🔧 Reparticionando en 4 particiones...\n",
      "2025-11-03 21:17:38,464 - SparkDataExporter - INFO - 🚀 Intento 1/3: Exportando a SQL Server → clean_fact_ventas\n",
      "2025-11-03 21:31:55,573 - SparkDataExporter - INFO - ✅✅✅ EXPORTACIÓN EXITOSA A SQL SERVER ✅✅✅\n",
      "2025-11-03 21:31:55,575 - SparkDataExporter - INFO - 📊 Total exportado: 40,000,000 registros\n",
      "2025-11-03 21:31:55,577 - SparkDataExporter - INFO - 📍 Tabla destino: clean_fact_ventas\n",
      "2025-11-03 21:31:55,578 - SparkDataExporter - INFO - ⚙️ Modo: overwrite\n",
      "2025-11-03 21:31:55,578 - SparkDataExporter - INFO - ⏱️ Tiempo total en export_to_sql: 868.62 segundos (14.48 minutos)\n",
      "2025-11-03 21:31:55,580 - SparkDataExporter - INFO - \n",
      "======================================================================\n",
      "2025-11-03 21:31:55,581 - SparkDataExporter - INFO - RESUMEN: 4/4 tablas exportadas exitosamente\n",
      "2025-11-03 21:31:55,581 - SparkDataExporter - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN DE EXPORTACIONES\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "                    RESUMEN DE EXPORTACIONES\n",
      "======================================================================\n",
      "\n",
      "clean_dim_tiempo:\n",
      "  Estado: ✓ Exitoso\n",
      "  Registros: 730\n",
      "  Modo: overwrite\n",
      "  Intentos: 1\n",
      "\n",
      "clean_dim_tienda:\n",
      "  Estado: ✓ Exitoso\n",
      "  Registros: 50\n",
      "  Modo: overwrite\n",
      "  Intentos: 1\n",
      "\n",
      "clean_dim_producto:\n",
      "  Estado: ✓ Exitoso\n",
      "  Registros: 100\n",
      "  Modo: overwrite\n",
      "  Intentos: 1\n",
      "\n",
      "clean_fact_ventas:\n",
      "  Estado: ✓ Exitoso\n",
      "  Registros: 40,000,000\n",
      "  Modo: overwrite\n",
      "  Intentos: 1\n",
      "======================================================================\n",
      "\n",
      "\n",
      "✓ Pipeline completado exitosamente\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Verificar resultados\u001b[39;00m\n\u001b[0;32m    115\u001b[0m all_sql_success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(results_sql\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 116\u001b[0m all_csv_success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(results_csv\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_sql_success \u001b[38;5;129;01mand\u001b[39;00m all_csv_success:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓✓✓ TODAS las exportaciones fueron exitosas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_csv' is not defined"
     ]
    }
   ],
   "source": [
    "from SparkDataLoader import SparkDataLoader\n",
    "from SparkDataCleaner import SparkDataCleaner\n",
    "from SparkDataExporter import SparkDataExporter\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN (AJUSTA ESTOS VALORES)\n",
    "# ============================================================================\n",
    "\n",
    "# Ruta al JAR (para cargar en Spark)\n",
    "JDBC_DRIVER_PATH = r\"C:\\sqljdbc_13.2.1.0_enu\\sqljdbc_13.2\\enu\\jars\\mssql-jdbc-13.2.1.jre8.jar\"\n",
    "\n",
    "# Conexión SQL Server\n",
    "JDBC_URL = r\"jdbc:sqlserver://ALTALU87:1433;databaseName=DB_Beverages;encrypt=true;trustServerCertificate=true\"\n",
    "JDBC_USER = os.getenv(\"SQL_DB_USER\")\n",
    "JDBC_PASSWORD =os.getenv(\"SQL_DB_PASS\")\n",
    "\n",
    "# Nombre de la clase del driver (NO CAMBIAR ESTO)\n",
    "JDBC_DRIVER_CLASS = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "# ============================================================================\n",
    "# PIPELINE COMPLETO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE: CARGA → LIMPIEZA → EXPORTACIÓN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PASO 1: Cargar datos CON el JAR\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n[1/4] Cargando datos...\")\n",
    "\n",
    "loader = SparkDataLoader(\n",
    "    app_name=\"PipelineCompleto\",\n",
    "    driver_memory=\"8g\"\n",
    ")\n",
    "\n",
    "# IMPORTANTE: Agregar el JAR manualmente a la sesión de Spark\n",
    "# loader.spark.sparkContext.addPyFile(JDBC_DRIVER_PATH)\n",
    "\n",
    "dirty_data = loader.load_all_tables(\n",
    "    base_path=\"./datos\",\n",
    "    filename_prefix=\"datos_bi_sucio_bebidas\"\n",
    ")\n",
    "\n",
    "loader.show_loaded_tables()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PASO 2: Limpiar datos\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n[2/4] Limpiando datos...\")\n",
    "\n",
    "cleaner = SparkDataCleaner(dataframes=dirty_data)\n",
    "\n",
    "clean_data = cleaner.clean_all(\n",
    "    enforce_integrity=True,\n",
    "    fact_options={\n",
    "        'remove_null_precio': False,\n",
    "        'remove_null_volumen': True,\n",
    "        'impute_distribucion': True,\n",
    "        'recalculate_valor': True,\n",
    "        'remove_outliers': False\n",
    "    }\n",
    ")\n",
    "\n",
    "cleaner.print_cleaning_summary()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PASO 3: Exportar a SQL Server\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n[3/4] Exportando a SQL Server...\")\n",
    "\n",
    "exporter = SparkDataExporter(\n",
    "    jdbc_url=JDBC_URL,\n",
    "    jdbc_user=JDBC_USER,\n",
    "    jdbc_password=JDBC_PASSWORD,\n",
    "    jdbc_driver=JDBC_DRIVER_CLASS  # ← NOMBRE DE CLASE (no la ruta)\n",
    ")\n",
    "\n",
    "# Exportar todas las tablas\n",
    "results_sql = exporter.export_multiple_tables_to_sql(\n",
    "    tables=clean_data,\n",
    "    table_prefix=\"clean_\",\n",
    "    mode=\"overwrite\",\n",
    "    batchsize=10000,\n",
    "    max_partitions=4,  # Ajusta según tu servidor\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "PASO 4: Exportar a CSV (backup)\n",
    "---------------------------------------------------------------------------\n",
    "print(\"\\n[4/4] Exportando a CSV...\")\n",
    "\n",
    "results_csv = exporter.export_multiple_tables_to_csv(\n",
    "    tables=clean_data,\n",
    "    base_path=\"./export_csv\",\n",
    "    num_files=1,\n",
    "    compression=None\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# RESUMEN FINAL\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE EXPORTACIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "exporter.print_export_summary()\n",
    "\n",
    "print(\"\\n✓ Pipeline completado exitosamente\")\n",
    "\n",
    "# Verificar resultados\n",
    "all_sql_success = all(results_sql.values())\n",
    "all_csv_success = all(results_csv.values())\n",
    "\n",
    "if all_sql_success and all_csv_success:\n",
    "    print(\"✓✓✓ TODAS las exportaciones fueron exitosas\")\n",
    "else:\n",
    "    if not all_sql_success:\n",
    "        failed_sql = [k for k, v in results_sql.items() if not v]\n",
    "        print(f\"⚠️ SQL fallidas: {failed_sql}\")\n",
    "    if not all_csv_success:\n",
    "        failed_csv = [k for k, v in results_csv.items() if not v]\n",
    "        print(f\"⚠️ CSV fallidas: {failed_csv}\")\n",
    "\n",
    "# Detener Spark\n",
    "loader.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd02ae-d935-44eb-a9bf-a403e45afe87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
